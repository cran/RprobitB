<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />



<title>Model definition</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>



<style type="text/css">
code {
white-space: pre;
}
.sourceCode {
overflow: visible;
}
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>



<style type="text/css">

div.csl-bib-body { }
div.csl-entry {
clear: both;
}
.hanging div.csl-entry {
margin-left:2em;
text-indent:-2em;
}
div.csl-left-margin {
min-width:2em;
float:left;
}
div.csl-right-inline {
margin-left:2em;
padding-left:1em;
}
div.csl-indent {
margin-left: 2em;
}
</style>

<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Model definition</h1>



<div id="the-probit-model" class="section level2">
<h2>The probit model</h2>
<p>The probit model<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> is a regression-type model where the
dependent variable only takes a finite number of values and the error
term is normally distributed <span class="citation">(<a href="#ref-Agresti2015">Agresti 2015</a>)</span>. Its purpose is to
estimate the probability that the dependent variable takes a certain,
discrete value. The most common application are discrete choice
scenarios. The dependent variable here is one of finitely many and
mutually exclusive alternatives, and explanatory variables typically are
characteristics of the deciders or the alternatives.</p>
<p>To be concrete, assume that we possess data of <span class="math inline">\(N\)</span> decision makers which choose between
<span class="math inline">\(J \geq 2\)</span> alternatives<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> at each of <span class="math inline">\(T\)</span> choice occasions<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>. Specific to each
decision maker, alternative and choice occasion, we furthermore observe
<span class="math inline">\(P\)</span> choice attributes that we use to
explain the choices. The continuous choice attributes cannot be linked
directly to the discrete choices but must take a detour over a latent
variable. In the discrete choice setting, this variable can be
interpreted as the decider’s utility of a certain alternative. Decider
<span class="math inline">\(n\)</span>’s utility <span class="math inline">\(U_{ntj}\)</span> for alternative <span class="math inline">\(j\)</span> at choice occasion <span class="math inline">\(t\)</span> is modeled as</p>
<p><span class="math display">\[\begin{equation}
  U_{ntj} = X_{ntj}&#39;\beta + \epsilon_{ntj}
\end{equation}\]</span></p>
<p>for <span class="math inline">\(n=1,\dots,N\)</span>, <span class="math inline">\(t=1,\dots,T\)</span> and <span class="math inline">\(j=1,\dots,J\)</span>, where</p>
<ul>
<li><p><span class="math inline">\(X_{ntj}\)</span> is a (column) vector
of <span class="math inline">\(P\)</span> characteristics of <span class="math inline">\(j\)</span> as faced by <span class="math inline">\(n\)</span> at <span class="math inline">\(t\)</span>,</p></li>
<li><p><span class="math inline">\(\beta \in {\mathbb R}^{P}\)</span> is
a vector of coefficients,</p></li>
<li><p>and <span class="math inline">\((\epsilon_{nt:}) =
(\epsilon_{nt1},\dots,\epsilon_{ntJ})&#39; \sim \text{MVN}_{J}
(0,\Sigma)\)</span> is the model’s error term vector for <span class="math inline">\(n\)</span> at <span class="math inline">\(t\)</span>, which in the probit model is assumed
to be multivariate normally distributed with zero mean and covariance
matrix <span class="math inline">\(\Sigma\)</span>.</p></li>
</ul>
<p>Now let <span class="math inline">\(y_{nt}=j\)</span> denote the
event that decision maker <span class="math inline">\(n\)</span> chooses
alternative <span class="math inline">\(j\)</span> at choice occasion
<span class="math inline">\(t\)</span>. Assuming utility maximizing
behavior of the decision makers<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>, the decisions are linked to the utilities
via</p>
<p><span class="math display">\[\begin{equation}
y_{nt} = {\arg \max}_{j = 1,\dots,J} U_{ntj}.
\end{equation}\]</span></p>
<p>In the ordered probit case, the concept of decider’s having separate
utilities for each alternative is no longer natural <span class="citation">(<a href="#ref-Train2009">Train 2009</a>)</span>.
Instead, we model only a single utility value <span class="math display">\[\begin{align*}
  U_{nt} = X_{nt}&#39;\beta_n + \epsilon_{nt}
\end{align*}\]</span> per decider <span class="math inline">\(n\)</span>
and choice occasion <span class="math inline">\(t\)</span>, which we
interpret as the “level of association” that <span class="math inline">\(n\)</span> has with the choice question. The
utility value falls into discrete categories, which in turn are linked
to the ordered alternatives <span class="math inline">\(j=1,\dots,J\)</span>. Formally, <span class="math display">\[\begin{align*}
   y_{nt} = \sum_{j = 1,\dots,J} j \cdot I(\gamma_{j-1} &lt; U_{nt} \leq
\gamma_{j}),
\end{align*}\]</span> with end points <span class="math inline">\(\gamma_0 = -\infty\)</span> and <span class="math inline">\(\gamma_J = +\infty\)</span>, and thresholds <span class="math inline">\((\gamma_j)_{j=1,\dots,J-1}\)</span>. To ensure
monotonicity of the thresholds, we rather estimate logarithmic threshold
increments <span class="math inline">\(d_j\)</span> with <span class="math inline">\(\gamma_j = \sum_{i=1,\dots,j} \exp{d_i}\)</span>,
<span class="math inline">\(j=1,\dots,J-1\)</span>.</p>
</div>
<div id="choice-behavior-heterogeneity" class="section level2">
<h2>Choice behavior heterogeneity</h2>
<p>Note that the coefficient vector <span class="math inline">\(\beta\)</span> is constant across decision makers.
This assumption is too restrictive for many applications.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> Heterogeneity in
choice behavior can be modeled by imposing a distribution on <span class="math inline">\(\beta\)</span> such that each decider can have
their own preferences.</p>
<p>Formally, we define <span class="math inline">\(\beta = (\alpha,
\beta_n)\)</span>, where <span class="math inline">\(\alpha\)</span> are
<span class="math inline">\(P_f\)</span> coefficients that are constant
across deciders and <span class="math inline">\(\beta_n\)</span> are
<span class="math inline">\(P_r\)</span> decider-specific coefficients.
Consequently, <span class="math inline">\(P = P_f + P_r\)</span>. Now if
<span class="math inline">\(P_r&gt;0\)</span>, <span class="math inline">\(\beta_n\)</span> is distributed according to some
<span class="math inline">\(P_r\)</span>-variate distribution, the
so-called mixing distribution.</p>
<p>Choosing an appropriate mixing distribution is a notoriously
difficult task of the model specification. In many applications,
different types of standard parametric distributions (including the
normal, log-normal, uniform and tent distribution) are tried in
conjunction with a likelihood value-based model selection, cf., <span class="citation">Train (<a href="#ref-Train2009">2009</a>)</span>,
Chapter 6. Instead, <code>{RprobitB}</code> implements the approach of
<span class="citation">(<a href="#ref-Oelschlaeger:2020"><strong>Oelschlaeger:2020?</strong></a>)</span>
to approximate any underlying mixing distribution by a mixture of
(multivariate) Gaussian densities. More precisely, the underlying mixing
distribution <span class="math inline">\(g_{P_r}\)</span> for the random
coefficients <span class="math inline">\((\beta_n)_{n}\)</span> is
approximated by a mixture of <span class="math inline">\(P_r\)</span>-variate normal densities <span class="math inline">\(\phi_{P_r}\)</span> with mean vectors <span class="math inline">\(b=(b_c)_{c}\)</span> and covariance matrices <span class="math inline">\(\Omega=(\Omega_c)_{c}\)</span> using <span class="math inline">\(C\)</span> components, i.e.</p>
<p><span class="math display">\[\begin{equation}
\beta_n\mid b,\Omega \sim \sum_{c=1}^{C} s_c \phi_{P_r} (\cdot \mid
b_c,\Omega_c).
\end{equation}\]</span></p>
<p>Here, <span class="math inline">\((s_c)_{c}\)</span> are weights
satisfying <span class="math inline">\(0 &lt; s_c\leq 1\)</span> for
<span class="math inline">\(c=1,\dots,C\)</span> and <span class="math inline">\(\sum_c s_c=1\)</span>. One interpretation of the
latent class model is obtained by introducing variables <span class="math inline">\(z=(z_n)_n\)</span>, allocating each decision maker
<span class="math inline">\(n\)</span> to class <span class="math inline">\(c\)</span> with probability <span class="math inline">\(s_c\)</span>, i.e.</p>
<p><span class="math display">\[\begin{equation}
\text{Prob}(z_n=c)=s_c \land \beta_n \mid z,b,\Omega \sim
\phi_{P_r}(\cdot \mid b_{z_n},\Omega_{z_n}).
\end{equation}\]</span></p>
<p>We call the resulting model the <em>latent class mixed multinomial
probit model</em>. Note that the model collapses to the <em>(normally)
mixed multinomial probit model</em> if <span class="math inline">\(P_r&gt;0\)</span> and <span class="math inline">\(C=1\)</span>, to the <em>multinomial probit
model</em> if <span class="math inline">\(P_r=0\)</span> and to the
<em>binary probit model</em> if additionally <span class="math inline">\(J=2\)</span>.</p>
</div>
<div id="model-normalization" class="section level2">
<h2>Model normalization</h2>
<p>As is well known, any utility model needs to be normalized with
respect to level and scale in order to be identified <span class="citation">(<a href="#ref-Train2009">Train 2009</a>)</span>.
Therefore, we consider the transformed model</p>
<p><span class="math display">\[\begin{equation}
\tilde{U}_{ntj} = \tilde{X}_{ntj}&#39; \beta + \tilde{\epsilon}_{ntj},
\end{equation}\]</span></p>
<p><span class="math inline">\(n=1,\dots,N\)</span>, <span class="math inline">\(t=1,\dots,T\)</span> and <span class="math inline">\(j=1,\dots,J-1\)</span>, where (choosing <span class="math inline">\(J\)</span> as the reference alternative) <span class="math inline">\(\tilde{U}_{ntj} = U_{ntj} - U_{ntJ}\)</span>,
<span class="math inline">\(\tilde{X}_{ntj} = X_{ntj} -
X_{ntJ}\)</span>, and <span class="math inline">\(\tilde{\epsilon}_{ntj}
= \epsilon_{ntj} - \epsilon_{ntJ}\)</span>, where <span class="math inline">\((\tilde{\epsilon}_{nt:}) =
(\tilde{\epsilon}_{nt1},...,\tilde{\epsilon}_{nt(J-1)})&#39; \sim
\text{MVN}_{J-1} (0,\tilde{\Sigma})\)</span> and <span class="math inline">\(\tilde{\Sigma}\)</span> denotes a covariance
matrix with the top-left element restricted to one.<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a></p>
</div>
<div id="parameter-labels" class="section level2">
<h2>Parameter labels</h2>
<p>In <code>{RprobitB}</code>, the probit model parameters are saved as
an <code>RprobitB_parameter</code> object. Their labels are consistent
with their definition in this vignette. For example:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a>RprobitB<span class="sc">:::</span><span class="fu">RprobitB_parameter</span>(</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>  <span class="at">P_f =</span> <span class="dv">1</span>,</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>  <span class="at">P_r =</span> <span class="dv">2</span>,</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a>  <span class="at">J =</span> <span class="dv">3</span>,</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>  <span class="at">N =</span> <span class="dv">10</span>,</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a>  <span class="at">C =</span> <span class="dv">2</span>, <span class="co"># the number of latent classes</span></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a>  <span class="at">alpha =</span> <span class="fu">c</span>(<span class="dv">1</span>), <span class="co"># the fixed coefficient vector of length &#39;P_f&#39;</span></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a>  <span class="at">s =</span> <span class="fu">c</span>(<span class="fl">0.6</span>, <span class="fl">0.4</span>), <span class="co"># the vector of class weights of length &#39;C&#39;</span></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a>  <span class="at">b =</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>), <span class="at">nrow =</span> <span class="dv">2</span>, <span class="at">ncol =</span> <span class="dv">2</span>),</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a>  <span class="co"># the matrix of class means as columns of dimension &#39;P_r&#39; x &#39;C&#39;</span></span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a>  <span class="at">Omega =</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="fu">diag</span>(<span class="dv">2</span>), <span class="fl">0.1</span> <span class="sc">*</span> <span class="fu">diag</span>(<span class="dv">2</span>)), <span class="at">nrow =</span> <span class="dv">4</span>, <span class="at">ncol =</span> <span class="dv">2</span>),</span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a>  <span class="co"># the matrix of class covariance matrices as columns of dimension &#39;P_r^2&#39; x &#39;C&#39;</span></span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a>  <span class="at">Sigma =</span> <span class="fu">diag</span>(<span class="dv">2</span>), <span class="co"># the differenced error term covariance matrix of dimension &#39;(J-1)&#39; x &#39;(J-1)&#39;</span></span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a>  <span class="co"># the undifferenced error term covariance matrix is labeled &#39;Sigma_full&#39;</span></span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a>  <span class="at">z =</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>, <span class="dv">5</span>) <span class="co"># the vector of the allocation variables of length &#39;N&#39;</span></span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a>)</span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a><span class="co">#&gt; alpha : 1</span></span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a><span class="co">#&gt; C : 2</span></span>
<span id="cb1-20"><a href="#cb1-20" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb1-21"><a href="#cb1-21" tabindex="-1"></a><span class="co">#&gt; s : double vector of length 2 </span></span>
<span id="cb1-22"><a href="#cb1-22" tabindex="-1"></a><span class="co">#&gt; 0.6 0.4</span></span>
<span id="cb1-23"><a href="#cb1-23" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb1-24"><a href="#cb1-24" tabindex="-1"></a><span class="co">#&gt; b : 2 x 2 matrix of doubles </span></span>
<span id="cb1-25"><a href="#cb1-25" tabindex="-1"></a><span class="co">#&gt;      [,1] [,2]</span></span>
<span id="cb1-26"><a href="#cb1-26" tabindex="-1"></a><span class="co">#&gt; [1,]   -1    1</span></span>
<span id="cb1-27"><a href="#cb1-27" tabindex="-1"></a><span class="co">#&gt; [2,]    1    2</span></span>
<span id="cb1-28"><a href="#cb1-28" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb1-29"><a href="#cb1-29" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb1-30"><a href="#cb1-30" tabindex="-1"></a><span class="co">#&gt; Omega : 4 x 2 matrix of doubles </span></span>
<span id="cb1-31"><a href="#cb1-31" tabindex="-1"></a><span class="co">#&gt;      [,1] [,2]</span></span>
<span id="cb1-32"><a href="#cb1-32" tabindex="-1"></a><span class="co">#&gt; [1,]    1  0.1</span></span>
<span id="cb1-33"><a href="#cb1-33" tabindex="-1"></a><span class="co">#&gt; [2,]    0    0</span></span>
<span id="cb1-34"><a href="#cb1-34" tabindex="-1"></a><span class="co">#&gt; [3,]    0    0</span></span>
<span id="cb1-35"><a href="#cb1-35" tabindex="-1"></a><span class="co">#&gt; [4,]    1  0.1</span></span>
<span id="cb1-36"><a href="#cb1-36" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb1-37"><a href="#cb1-37" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb1-38"><a href="#cb1-38" tabindex="-1"></a><span class="co">#&gt; Sigma : 2 x 2 matrix of doubles </span></span>
<span id="cb1-39"><a href="#cb1-39" tabindex="-1"></a><span class="co">#&gt;      [,1] [,2]</span></span>
<span id="cb1-40"><a href="#cb1-40" tabindex="-1"></a><span class="co">#&gt; [1,]    1    0</span></span>
<span id="cb1-41"><a href="#cb1-41" tabindex="-1"></a><span class="co">#&gt; [2,]    0    1</span></span>
<span id="cb1-42"><a href="#cb1-42" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb1-43"><a href="#cb1-43" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb1-44"><a href="#cb1-44" tabindex="-1"></a><span class="co">#&gt; Sigma_full : 3 x 3 matrix of doubles </span></span>
<span id="cb1-45"><a href="#cb1-45" tabindex="-1"></a><span class="co">#&gt;      [,1] [,2] [,3]</span></span>
<span id="cb1-46"><a href="#cb1-46" tabindex="-1"></a><span class="co">#&gt; [1,]    2    1    1</span></span>
<span id="cb1-47"><a href="#cb1-47" tabindex="-1"></a><span class="co">#&gt; [2,]    1    2    1</span></span>
<span id="cb1-48"><a href="#cb1-48" tabindex="-1"></a><span class="co">#&gt; [3,]    1    1    1</span></span>
<span id="cb1-49"><a href="#cb1-49" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb1-50"><a href="#cb1-50" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb1-51"><a href="#cb1-51" tabindex="-1"></a><span class="co">#&gt; beta : 2 x 10 matrix of doubles </span></span>
<span id="cb1-52"><a href="#cb1-52" tabindex="-1"></a><span class="co">#&gt;       [,1] [,2] [,3] ... [,10]</span></span>
<span id="cb1-53"><a href="#cb1-53" tabindex="-1"></a><span class="co">#&gt; [1,] -0.76 1.14 -0.7 ...  0.52</span></span>
<span id="cb1-54"><a href="#cb1-54" tabindex="-1"></a><span class="co">#&gt; [2,]  3.15 2.22 0.75 ...   2.7</span></span>
<span id="cb1-55"><a href="#cb1-55" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb1-56"><a href="#cb1-56" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb1-57"><a href="#cb1-57" tabindex="-1"></a><span class="co">#&gt; z : double vector of length 10 </span></span>
<span id="cb1-58"><a href="#cb1-58" tabindex="-1"></a><span class="co">#&gt; 1 2 1 ... 2</span></span>
<span id="cb1-59"><a href="#cb1-59" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb1-60"><a href="#cb1-60" tabindex="-1"></a><span class="co">#&gt; d : NA</span></span></code></pre></div>
<p>Mind that the matrix <code>Sigma_full</code> is not unique and can be
any matrix that results into <code>Sigma</code> after the differencing,
see the non-exported function
<code>RprobitB:::undiff_Sigma()</code>.</p>
</div>
<div id="references" class="section level2 unnumbered">
<h2 class="unnumbered">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Agresti2015" class="csl-entry">
Agresti, A. 2015. <em>Foundations of Linear and Generalized Linear
Models</em>. Wiley.
</div>
<div id="ref-Bliss1934" class="csl-entry">
Bliss, C. I. 1934. <span>“The Method of Probits.”</span>
<em>Science</em> 79 (2037). <a href="https://doi.org/10.1126/science.79.2037.38">https://doi.org/10.1126/science.79.2037.38</a>.
</div>
<div id="ref-Hewig2011" class="csl-entry">
Hewig, J., N. Kretschmer, R. H. Trippe, H. Hecht, M. G. H. Coles, C. B.
Holroyd, and W. H. R. Miltner. 2011. <span>“Why Humans Deviate from
Rational Choice.”</span> <em>Psychophysiology</em> 48 (4). <a href="https://doi.org/10.1111/j.1469-8986.2010.01081.x">https://doi.org/10.1111/j.1469-8986.2010.01081.x</a>.
</div>
<div id="ref-Train2009" class="csl-entry">
Train, K. 2009. <em>Discrete Choice Methods with Simulation</em>.
Cambridge University Press. <a href="https://doi.org/10.1017/CBO9780511805271">https://doi.org/10.1017/CBO9780511805271</a>.
</div>
</div>
</div>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p>The name <em>probit</em> is a portmanteau of
<em>prob</em>ability and un<em>it</em>. <span class="citation">(<a href="#ref-Bliss1934">Bliss 1934</a>)</span><a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>To be precise, the model name gets the prefix
<em>multinomial</em> in the case <span class="math inline">\(J &gt;
2\)</span>.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>For notational simplicity, the number of choice
occasions <span class="math inline">\(T\)</span> is assumed to be the
same for each decision maker here. However, we are not restricted to
this case: <code>{RprobitB}</code> allows for unbalanced panels,
i.e. varying <span class="math inline">\(T\)</span>. Of course, the
cross-sectional case <span class="math inline">\(T = 1\)</span> is
possible.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>This in fact is a critical assumption because many
studies show that humans do not decide in this rational sense in
general, see for example <span class="citation">Hewig et al. (<a href="#ref-Hewig2011">2011</a>)</span><a href="#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>For example, consider the case of modeling the choice of
a means of transportation to work: It is easily imaginable that business
people and pensioners do not share the same sensitivities towards cost
and time.<a href="#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p><code>{RprobitB}</code> provides an alternative to
fixing an error term variance in order to normalize with respect to
scale by fixing an element of <span class="math inline">\(\alpha\)</span>.<a href="#fnref6" class="footnote-back">↩︎</a></p></li>
</ol>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
