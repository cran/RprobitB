<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Lennart Oelschläger" />

<meta name="date" content="2021-11-12" />

<title>Model fitting</title>

<script src="data:application/javascript;base64,Ly8gUGFuZG9jIDIuOSBhZGRzIGF0dHJpYnV0ZXMgb24gYm90aCBoZWFkZXIgYW5kIGRpdi4gV2UgcmVtb3ZlIHRoZSBmb3JtZXIgKHRvCi8vIGJlIGNvbXBhdGlibGUgd2l0aCB0aGUgYmVoYXZpb3Igb2YgUGFuZG9jIDwgMi44KS4KZG9jdW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignRE9NQ29udGVudExvYWRlZCcsIGZ1bmN0aW9uKGUpIHsKICB2YXIgaHMgPSBkb2N1bWVudC5xdWVyeVNlbGVjdG9yQWxsKCJkaXYuc2VjdGlvbltjbGFzcyo9J2xldmVsJ10gPiA6Zmlyc3QtY2hpbGQiKTsKICB2YXIgaSwgaCwgYTsKICBmb3IgKGkgPSAwOyBpIDwgaHMubGVuZ3RoOyBpKyspIHsKICAgIGggPSBoc1tpXTsKICAgIGlmICghL15oWzEtNl0kL2kudGVzdChoLnRhZ05hbWUpKSBjb250aW51ZTsgIC8vIGl0IHNob3VsZCBiZSBhIGhlYWRlciBoMS1oNgogICAgYSA9IGguYXR0cmlidXRlczsKICAgIHdoaWxlIChhLmxlbmd0aCA+IDApIGgucmVtb3ZlQXR0cmlidXRlKGFbMF0ubmFtZSk7CiAgfQp9KTsK"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>


<style type="text/css">
  code {
    white-space: pre;
  }
  .sourceCode {
    overflow: visible;
  }
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>



<style type="text/css">
/* for pandoc --citeproc since 2.11 */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="data:text/css,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" type="text/css" />




</head>

<body>




<h1 class="title toc-ignore">Model fitting</h1>
<h4 class="author">Lennart Oelschläger</h4>
<h4 class="date">2021-11-12</h4>



<p><strong>RprobitB</strong> estimates a (latent class) (mixed) (multinomial) probit model in a <a href="#bayes-estimation-of-the-probit-model-via-gibbs-sampling">Bayesian framework via Gibbs sampling</a>.</p>
<p>To fit a model to choice data, apply the function</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">=</span> <span class="fu">mcmc</span>(<span class="at">data =</span> data)</span></code></pre></div>
<p>where <code>data</code> must be the output of either <code>prepare()</code> or <code>simulate()</code>, see <a href="data_management.html">the vignette about data management</a>.</p>
<p>The function <code>mcmc()</code> has the following optional arguments:</p>
<ul>
<li><p><code>scale</code>: A named list of three elements, determining the parameter normalization with respect to the utility scale (see <a href="introduction_to_RprobitB_and_model_formulation.html">the introductory vignette</a>)<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>:</p>
<ul>
<li><p><code>parameter</code>: Either <code>&quot;a&quot;</code> (for a linear coefficient of <code>&quot;alpha&quot;</code>) or <code>&quot;s&quot;</code> (for a variance of the error-term covariance matrix <code>&quot;Sigma&quot;</code>).</p></li>
<li><p><code>index</code>: The index of the parameter that gets fixed.</p></li>
<li><p><code>value</code>: The value for the fixed parameter.</p></li>
</ul></li>
<li><p><code>R</code>: The number of iterations of the Gibbs sampler.</p></li>
<li><p><code>B</code>: The length of the burn-in period, i.e. a non-negative number of samples to be discarded. See <a href="#burning-and-thinning">below</a> for details.</p></li>
<li><p><code>Q</code>: The thinning factor for the Gibbs samples, i.e. only every <code>Q</code>th sample is kept. See <a href="#burning-and-thinning">below</a> for details.</p></li>
<li><p><code>print_progress</code>: A boolean, determining whether to print the Gibbs sampler progress and the estimated remaining computation time.</p></li>
<li><p><code>prior</code>: A named list of parameters for the <a href="#prior-settings">prior distributions of the normalized parameters</a>:</p>
<ul>
<li><p><code>eta</code>: The mean vector of length <code>P_f</code> of the normal prior for <code>alpha</code>.</p></li>
<li><p><code>Psi</code>: The covariance matrix of dimension <code>P_f</code> x <code>P_f</code> of the normal prior for <code>alpha</code>.</p></li>
<li><p><code>delta</code>: The concentration parameter of length 1 of the Dirichlet prior for <code>s</code>.</p></li>
<li><p><code>xi</code>: The mean vector of length <code>P_r</code> of the normal prior for each <code>b_c</code>.</p></li>
<li><p><code>D</code>: The covariance matrix of dimension <code>P_r</code> x <code>P_r</code> of the normal prior for each <code>b_c</code>.</p></li>
<li><p><code>nu</code>: The degrees of freedom (a natural number greater than <code>P_r</code>) of the Inverse Wishart prior for each <code>Omega_c</code>.</p></li>
<li><p><code>Theta</code>: The scale matrix of dimension <code>P_r</code> x <code>P_r</code> of the Inverse Wishart prior for each <code>Omega_c</code>.</p></li>
<li><p><code>kappa</code>: The degrees of freedom (a natural number greater than <code>J-1</code>) of the Inverse Wishart prior for <code>Sigma</code>.</p></li>
<li><p><code>E</code>: The scale matrix of dimension <code>J-1</code> x <code>J-1</code> of the Inverse Wishart prior for <code>Sigma</code>.</p></li>
</ul></li>
<li><p><code>latent_classes</code>: A list of parameters specifying the number and <a href="#updating-the-number-of-latent-classes">the updating scheme of latent classes</a>:</p>
<ul>
<li><p><code>C</code>: The number (greater or equal 1) of latent classes. Set to 1 per default and is ignored if <code>P_r = 0</code></p></li>
<li><p><code>update</code>: A boolean, determining whether to update <code>C</code>. Ignored if <code>P_r = 0</code>. If <code>update = FALSE</code>, all of the following elements are ignored.</p></li>
<li><p><code>Cmax</code>: The maximum number of latent classes.</p></li>
<li><p><code>buffer</code>: The updating buffer (number of iterations to wait before the next update).</p></li>
<li><p><code>epsmin</code>: The threshold weight for removing latent classes (between 0 and 1).</p></li>
<li><p><code>epsmax</code>: The threshold weight for splitting latent classes (between 0 and 1).</p></li>
<li><p><code>distmin</code>: The threshold difference in means for joining latent classes (non-negative).</p></li>
</ul></li>
</ul>
<div id="prior-settings" class="section level2">
<h2>Prior settings</h2>
<p>Bayesian analysis enables to impose prior beliefs on the model parameters. It is possible to either express strong prior knowledge using informative prior distributions or to express vague knowledge using diffuse prior distributions. <strong>RprobitB</strong> applies the following conjugate priors:</p>
<ul>
<li><p><span class="math inline">\((s_1,\dots,s_C)\sim D_C(\delta)\)</span>, where <span class="math inline">\(D_C(\delta)\)</span> denotes the <span class="math inline">\(C\)</span>-dimensional Dirichlet distribution with concentration parameter vector <span class="math inline">\(\delta = (\delta_1,\dots,\delta_C)\)</span>,</p></li>
<li><p><span class="math inline">\(\alpha\sim \text{MVN}{P_f}(\psi,\Psi)\)</span>,</p></li>
<li><p><span class="math inline">\(b_c \sim \text{MVN}{P_r}(\xi,\Xi)\)</span>, independent for all <span class="math inline">\(c\)</span>,</p></li>
<li><p><span class="math inline">\(\Omega_c \sim W^{-1}_{P_r}(\nu,\Theta)\)</span>, independent for all <span class="math inline">\(c\)</span>, where <span class="math inline">\(W^{-1}_{P_r}(\nu,\Theta)\)</span> denotes the <span class="math inline">\(P_r\)</span>-dimensional inverse Wishart distribution with <span class="math inline">\(\nu\)</span> degrees of freedom and scale matrix <span class="math inline">\(\Theta\)</span>,</p></li>
<li><p>and <span class="math inline">\(\Sigma \sim W^{-1}_{J-1}(\kappa,\Lambda)\)</span>.</p></li>
</ul>
<p>Per default, <strong>RprobitB</strong> applies the diffuse prior approach, setting <span class="math inline">\(\delta_1=\dots=\delta_C=1\)</span>; <span class="math inline">\(\psi\)</span> and <span class="math inline">\(\xi\)</span> equal to the zero vector; <span class="math inline">\(\Psi\)</span> and <span class="math inline">\(\Xi\)</span> equal to the identity matrix; <span class="math inline">\(\nu\)</span> and <span class="math inline">\(\kappa\)</span> equal to <span class="math inline">\(P_r+2\)</span> and <span class="math inline">\(J+1\)</span>, respectively (to obtain proper priors); <span class="math inline">\(\Theta\)</span> and <span class="math inline">\(\Lambda\)</span> equal to the identity matrix.</p>
<p>Alternatively, the parameters can be chosen based on estimation results of similar choice settings, resulting in informative priors.</p>
</div>
<div id="bayes-estimation-of-the-probit-model-via-gibbs-sampling" class="section level2">
<h2>Bayes estimation of the probit model via Gibbs sampling</h2>
<p>The Bayesian analysis of the (latent class) (mixed) (multinomial) probit model builds upon the work of <span class="citation">(McCulloch and Rossi 1994)</span>, <span class="citation">(Nobile 1998)</span>, <span class="citation">(Allenby and Rossi 1998)</span>, and <span class="citation">(Imai and Dyk 2005)</span>. A key ingredient is the concept of data augmentation, cf. <span class="citation">(Albert and Chib 1993)</span>, which treats the latent utilities as parameters themselves. Conditional on the latent utilities, the multinomial probit model constitutes a standard Bayesian linear regression set-up, which renders drawing from the posterior distribution feasible without the need to evaluate any likelihood.</p>
<p>Gibbs sampling from the joint posterior distribution of a latent class mixed multinomial probit model proceeds by iteratively drawing and updating each model parameter conditional on the other parameters.</p>
<ul>
<li><p>The class weights are drawn from the Dirichlet distribution <span class="math display">\[\begin{equation}
(s_1,\dots,s_C)\mid \delta,z \sim D_C(\delta_1+m_1,\dots,\delta_C+m_C),
\end{equation}\]</span> where for <span class="math inline">\(c=1,\dots,C\)</span>, <span class="math inline">\(m_c=\#\{n:z_n=c\}\)</span> denotes the current absolute class size. Mind that the model is invariant to permutations of the class labels <span class="math inline">\(1,\dots,C\)</span>. For that reason, we accept an update only if the ordering <span class="math inline">\(s_1&lt;\dots&lt;s_C\)</span> holds, thereby ensuring a unique labeling of the classes.</p></li>
<li><p>Independently for all <span class="math inline">\(n\)</span>, we update the allocation variables <span class="math inline">\((z_n)_n\)</span> from their conditional distribution <span class="math display">\[\begin{equation}
\text{Prob}(z_n=c\mid s,\beta,b,\Omega )=\frac{s_c\phi_{P_r}(\beta_n\mid b_c,\Omega_c)}{\sum_c s_c\phi_{P_r}(\beta_n\mid b_c,\Omega_c)}.
\end{equation}\]</span></p></li>
<li><p>The class means <span class="math inline">\((b_c)_c\)</span> are updated independently for all <span class="math inline">\(c\)</span> via <span class="math display">\[\begin{equation}
b_c\mid \Xi,\Omega,\xi,z,\beta \sim\text{MVN}{P_r}\left( \mu_{b_c}, \Sigma_{b_c}  \right),
\end{equation}\]</span> where <span class="math inline">\(\mu_{b_c}=(\Xi^{-1}+m_c\Omega_c^{-1})^{-1}(\Xi^{-1}\xi +m_c\Omega_c^{-1}\bar{b}_c)\)</span>, <span class="math inline">\(\Sigma_{b_c}=(\Xi^{-1}+m_c\Omega_c^{-1})^{-1}\)</span>, <span class="math inline">\(\bar{b}_c=m_c^{-1}\sum_{n:z_n=c} \beta_n\)</span>.</p></li>
<li><p>The class covariance matrices <span class="math inline">\((\Omega_c)_c\)</span> are updated independently for all <span class="math inline">\(c\)</span> via <span class="math display">\[\begin{equation}
\Omega_c \mid \nu,\Theta,z,\beta,b \sim W^{-1}_{P_r}(\mu_{\Omega_c},\Sigma_{\Omega_c}),
\end{equation}\]</span> where <span class="math inline">\(\mu_{\Omega_c}=\nu+m_c\)</span> and <span class="math inline">\(\Sigma_{\Omega_c}=\Theta^{-1} + \sum_{n:z_n=c} (\beta_n-b_c)(\beta_n-b_c)&#39;\)</span>.</p></li>
<li><p>Independently for all <span class="math inline">\(n\)</span> and <span class="math inline">\(t\)</span> and conditionally on the other components, the utility vectors <span class="math inline">\((U_{nt:})\)</span> follow a <span class="math inline">\(J-1\)</span>-dimensional truncated multivariate normal distribution, where the truncation points are determined by the choices <span class="math inline">\(y_{nt}\)</span>. To sample from a truncated multivariate normal distribution, we apply a sub-Gibbs sampler, following the approach of : <span class="math display">\[\begin{equation}
U_{ntj} \mid U_{nt(-j)},y_{nt},\Sigma,W,\alpha,X,\beta 
\sim \mathcal{N}(\mu_{U_{ntj}},\Sigma_{U_{ntj}}) \cdot \begin{cases}
1(U_{ntj}&gt;\max(U_{nt(-j)},0) ) &amp; \text{if}~ y_{nt}=j\\
1(U_{ntj}&lt;\max(U_{nt(-j)},0) ) &amp; \text{if}~ y_{nt}\neq j
\end{cases},
\end{equation}\]</span> where <span class="math inline">\(U_{nt(-j)}\)</span> denotes the vector <span class="math inline">\((U_{nt:})\)</span> without the element <span class="math inline">\(U_{ntj}\)</span>, <span class="math inline">\(\mathcal{N}\)</span> denotes the univariate normal distribution, <span class="math inline">\(\Sigma_{U_{ntj}} = 1/(\Sigma^{-1})_{jj}\)</span> and <span class="math display">\[\begin{equation}
\mu_{U_{ntj}} = W_{ntj}&#39;\alpha + X_{ntj}&#39;\beta_n - \Sigma_{U_{ntj}} (\Sigma^{-1})_{j(-j)}   (U_{nt(-j)} - W_{nt(-j)}&#39;\alpha - X_{nt(-j)}&#39; \beta_n ),
\end{equation}\]</span> where <span class="math inline">\((\Sigma^{-1})_{jj}\)</span> denotes the <span class="math inline">\((j,j)\)</span>th element of <span class="math inline">\(\Sigma^{-1}\)</span>, <span class="math inline">\((\Sigma^{-1})_{j(-j)}\)</span> the <span class="math inline">\(j\)</span>th row without the <span class="math inline">\(j\)</span>th entry, <span class="math inline">\(W_{nt(-j)}\)</span> and <span class="math inline">\(X_{nt(-j)}\)</span> the coefficient matrices <span class="math inline">\(W_{nt}\)</span> and <span class="math inline">\(X_{nt}\)</span>, respectively, without the <span class="math inline">\(j\)</span>th column.</p></li>
<li><p>Updating the fixed coefficient vector <span class="math inline">\(\alpha\)</span> is achieved by applying the formula for Bayesian linear regression of the regressors <span class="math inline">\(W_{nt}\)</span> on the regressands <span class="math inline">\((U_{nt:})-X_{nt}&#39;\beta_n\)</span>, i.e. <span class="math display">\[\begin{equation}
\alpha \mid \Psi,\psi,W,\Sigma,U,X,\beta \sim \text{MVN}{P_f}(\mu_\alpha,\Sigma_\alpha),
\end{equation}\]</span> where <span class="math inline">\(\mu_\alpha = \Sigma_\alpha (\Psi^{-1}\psi + \sum_{n=1,t=1}^{N,T} W_{nt} \Sigma^{-1} ((U_{nt:})-X_{nt}&#39;\beta_n) )\)</span> and <span class="math inline">\(\Sigma_\alpha = (\Psi^{-1} + \sum_{n=1,t=1}^{N,T} W_{nt}\Sigma^{-1} W_{nt}^{&#39;} )^{-1}\)</span>.</p></li>
<li><p>Analogously to <span class="math inline">\(\alpha\)</span>, the random coefficients <span class="math inline">\((\beta_n)_n\)</span> are updated independently via <span class="math display">\[\begin{equation}
\beta_n \mid \Omega,b,X,\Sigma,U,W,\alpha \sim \text{MVN}{P_r}(\mu_{\beta_n},\Sigma_{\beta_n}),
\end{equation}\]</span> where <span class="math inline">\(\mu_{\beta_n} = \Sigma_{\beta_n} (\Omega_{z_n}^{-1}b_{z_n} + \sum_{t=1}^{T} X_{nt} \Sigma^{-1} (U_{nt}-W_{nt}&#39;\alpha) )\)</span> and <span class="math inline">\(\Sigma_{\beta_n} = (\Omega_{z_n}^{-1} + \sum_{t=1}^{T} X_{nt}\Sigma^{-1} X_{nt}^{&#39;} )^{-1}\)</span> .</p></li>
<li><p>The error term covariance matrix <span class="math inline">\(\Sigma\)</span> is updated by means of <span class="math display">\[\begin{equation}
\Sigma \mid \kappa,\Lambda,U,W,\alpha,X,\beta \sim W^{-1}_{J-1}(\kappa+NT,\Lambda+S), \\
\end{equation}\]</span> where <span class="math inline">\(S = \sum_{n=1,t=1}^{N,T} \varepsilon_{nt} \varepsilon_{nt}&#39;\)</span> and <span class="math inline">\(\varepsilon_{nt} = (U_{nt:}) - W_{nt}&#39;\alpha - X_{nt}&#39;\beta_n\)</span>.</p></li>
</ul>
</div>
<div id="parameter-normalization" class="section level2">
<h2>Parameter normalization</h2>
<p>Samples obtained from the scheme described above still lack identification (see <a href="introduction_to_RprobitB_and_model_formulation.html">the introductory vignette</a>). Therefore, subsequent to the sampling, the normalizations</p>
<ul>
<li><p><span class="math inline">\(\alpha^{(i)}/\sqrt{(\Sigma^{(i)})_{11}}\)</span>,</p></li>
<li><p><span class="math inline">\(b_c^{(i)}/\sqrt{(\Sigma^{(i)})_{11}}\)</span>,</p></li>
<li><p><span class="math inline">\(\Omega_c^{(i)}/(\Sigma^{(i)})_{11}\)</span>, <span class="math inline">\(c=1,\dots,C\)</span> and</p></li>
<li><p><span class="math inline">\(\Sigma^{(i)}/(\Sigma^{(i)})_{11}\)</span></p></li>
</ul>
<p>are required for the <span class="math inline">\(i\)</span>th updates in each iterations <span class="math inline">\(i\)</span>, cf. <span class="citation">(Imai and Dyk 2005)</span>, where <span class="math inline">\((\Sigma^{(i)})_{11}\)</span> denotes the top-left element of <span class="math inline">\(\Sigma^{(i)}\)</span>.</p>
<p>The draws for <span class="math inline">\(s\)</span> and <span class="math inline">\(z\)</span> do not need to be normalized. The draws for <span class="math inline">\(U\)</span> and <span class="math inline">\(\beta\)</span> could be normalized if the results are of interest in the analysis.</p>
<p>Alternatively, the samples can be normalized such that any variance of <span class="math inline">\(\Sigma\)</span> or any element of <span class="math inline">\(\alpha\)</span> equals any fixed non-negative value.</p>
<p>The normalization of a fitted model can be changed afterwards via</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">=</span> <span class="fu">transform</span>(<span class="at">model =</span> model, <span class="at">scale =</span> scale)</span></code></pre></div>
<p>where <code>model</code> is the output of <code>mcmc()</code> and <code>scale</code> is a named list of three elements, determining the parameter normalization, as described above.</p>
</div>
<div id="burning-and-thinning" class="section level2">
<h2>Burning and thinning</h2>
<p>The theory behind Gibbs sampling constitutes that the sequence of samples produced by the updating scheme can be considered as a Markov chain with stationary distribution equal to the desired joint posterior distribution. It takes a certain number of iterations for that stationary distribution to be approximated reasonably well. Therefore, it is common practice to discard the first <span class="math inline">\(B\)</span> out of <span class="math inline">\(R\)</span> samples (the so-called burn-in period). Furthermore, correlation between nearby samples should be expected. In order to obtain independent samples, we consider only every <span class="math inline">\(Q\)</span>th sample when averaging values to compute parameter statistics like expectation and standard deviation.</p>
<p>Adequate values for <span class="math inline">\(R\)</span>, <span class="math inline">\(B\)</span> and <span class="math inline">\(Q\)</span> depend on the complexity of the considered Bayesian framework. Per default, <strong>RprobitB</strong> sets <code>R = 1e4</code>, <code>B = R/2</code> and <code>Q = 10</code>.</p>
<p>The independence of the samples can be verified by computing the serial correlation and the convergence of the Gibbs sampler can be checked by considering trace plots.</p>
</div>
<div id="updating-the-number-of-latent-classes" class="section level2">
<h2>Updating the number of latent classes</h2>
<p>Updating the number <span class="math inline">\(C\)</span> of latent classes is done within the Gibbs sampler by executing the following weight-based updating scheme within the second half of the burn-in period<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>:</p>
<ul>
<li><p>We remove class <span class="math inline">\(c\)</span>, if <span class="math inline">\(s_c&lt;\varepsilon_{\text{min}}\)</span>, i.e. if the class weight <span class="math inline">\(s_c\)</span> drops below some threshold <span class="math inline">\(\varepsilon_{\text{min}}\)</span>. This case indicates that class <span class="math inline">\(c\)</span> has a negligible impact on the mixing distribution.</p></li>
<li><p>We split class <span class="math inline">\(c\)</span> into two classes <span class="math inline">\(c_1\)</span> and <span class="math inline">\(c_2\)</span>, if <span class="math inline">\(s_c&gt;\varepsilon_\text{max}\)</span>. This case indicates that class <span class="math inline">\(c\)</span> has a high influence on the mixing distribution whose approximation can potentially be improved by increasing the resolution in directions of high variance. Therefore, the class means <span class="math inline">\(b_{c_1}\)</span> and <span class="math inline">\(b_{c_2}\)</span> of the new classes <span class="math inline">\(c_1\)</span> and <span class="math inline">\(c_2\)</span> are shifted in opposite directions from the class mean <span class="math inline">\(b_c\)</span> of the old class <span class="math inline">\(c\)</span> in the direction of the highest variance.</p></li>
<li><p>We join two classes <span class="math inline">\(c_1\)</span> and <span class="math inline">\(c_2\)</span> to one class <span class="math inline">\(c\)</span>, if <span class="math inline">\(\lVert b_{c_1} - b_{c_2} \rVert&lt;\varepsilon_{\text{distmin}}\)</span>, i.e. if the euclidean distance between the class means <span class="math inline">\(b_{c_1}\)</span> and <span class="math inline">\(b_{c_2}\)</span> drops below some threshold <span class="math inline">\(\varepsilon_{\text{distmin}}\)</span>. This case indicates location redundancy which should be repealed. The parameters of <span class="math inline">\(c\)</span> are assigned by adding the values of <span class="math inline">\(s\)</span> from <span class="math inline">\(c_1\)</span> and <span class="math inline">\(c_2\)</span> and averaging the values for <span class="math inline">\(b\)</span> and <span class="math inline">\(\Omega\)</span>.</p></li>
</ul>
<p>These rules contain choices on the values for <span class="math inline">\(\varepsilon_{\text{min}}\)</span>, <span class="math inline">\(\varepsilon_{\text{max}}\)</span> and <span class="math inline">\(\varepsilon_{\text{distmin}}\)</span>. The adequate value for <span class="math inline">\(\varepsilon_{\text{distmin}}\)</span> depends on the scale of the parameters. Per default, <strong>RprobitB</strong> sets</p>
<ul>
<li><p><code>epsmin = 0.01</code>,</p></li>
<li><p><code>epsmax = 0.99</code>, and</p></li>
<li><p><code>distmin = 0.1</code>.</p></li>
</ul>
</div>
<div id="examples" class="section level2">
<h2>Examples</h2>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="do">### probit model</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>p <span class="ot">=</span> <span class="fu">simulate</span>(<span class="at">form =</span> choice <span class="sc">~</span> var <span class="sc">|</span> <span class="dv">0</span>, <span class="at">N =</span> <span class="dv">100</span>, <span class="at">T =</span> <span class="dv">10</span>, <span class="at">J =</span> <span class="dv">2</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>m1 <span class="ot">=</span> <span class="fu">mcmc</span>(<span class="at">data =</span> p)</span></code></pre></div>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="do">### multinomial probit model</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>mnp <span class="ot">=</span> <span class="fu">simulate</span>(<span class="at">form =</span> choice <span class="sc">~</span> var <span class="sc">|</span> <span class="dv">0</span>, <span class="at">N =</span> <span class="dv">100</span>, <span class="at">T =</span> <span class="dv">10</span>, <span class="at">J =</span> <span class="dv">3</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>m2 <span class="ot">=</span> <span class="fu">mcmc</span>(<span class="at">data =</span> mnp)</span></code></pre></div>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="do">### mixed multinomial probit model</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>mmnp <span class="ot">=</span> <span class="fu">simulate</span>(<span class="at">form =</span> choice <span class="sc">~</span> <span class="dv">0</span> <span class="sc">|</span> var, <span class="at">N =</span> <span class="dv">100</span>, <span class="at">T =</span> <span class="dv">10</span>, <span class="at">J =</span> <span class="dv">3</span>, <span class="at">re =</span> <span class="st">&quot;var&quot;</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>m3 <span class="ot">=</span> <span class="fu">mcmc</span>(<span class="at">data =</span> mmnp)</span></code></pre></div>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="do">### latent classes mixed multinomial probit model</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>lcmmnp <span class="ot">=</span> <span class="fu">simulate</span>(<span class="at">form =</span> choice <span class="sc">~</span> <span class="dv">0</span> <span class="sc">|</span> var, <span class="at">N =</span> <span class="dv">100</span>, <span class="at">T =</span> <span class="dv">10</span>, <span class="at">J =</span> <span class="dv">3</span>, <span class="at">re =</span> <span class="st">&quot;var&quot;</span>,</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>                  <span class="at">parm =</span> <span class="fu">list</span>(<span class="st">&quot;C&quot;</span> <span class="ot">=</span> <span class="dv">2</span>))</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>m4 <span class="ot">=</span> <span class="fu">mcmc</span>(<span class="at">data =</span> lcmmnp, <span class="at">latent_classes =</span> <span class="fu">list</span>(<span class="st">&quot;C&quot;</span> <span class="ot">=</span> <span class="dv">2</span>))</span></code></pre></div>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="do">### update of latent classes</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>m5 <span class="ot">=</span> <span class="fu">mcmc</span>(<span class="at">data =</span> lcmmnp, <span class="at">latent_classes =</span> <span class="fu">list</span>(<span class="st">&quot;update&quot;</span> <span class="ot">=</span> <span class="cn">TRUE</span>))</span></code></pre></div>
</div>
<div id="references" class="section level2 unnumbered">
<h2 class="unnumbered">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Albert:93" class="csl-entry">
Albert, James H., and Siddhartha Chib. 1993. <span>“Bayesian Analysis of Binary and Polychotomous Response Data.”</span> <em>Journal of the American Statistical Association</em> 88.
</div>
<div id="ref-Allenby:98" class="csl-entry">
Allenby, Greg M., and Peter Rossi. 1998. <span>“Marketing Models of Consumer Heterogeneity.”</span> <em>Journal of Econometrics</em> 89.
</div>
<div id="ref-Imai:05" class="csl-entry">
Imai, Kosuke, and David A. van Dyk. 2005. <span>“A Bayesian Analysis of the Multinomial Probit Model Using Marginal Data Augmentation.”</span> <em>Journal of Econometrics</em> 124.
</div>
<div id="ref-McCulloch:94" class="csl-entry">
McCulloch, Robert, and Peter Rossi. 1994. <span>“An Exact Likelihood Analysis of the Multinomial Probit Model.”</span> <em>Journal of Econometrics</em> 64.
</div>
<div id="ref-Nobile:98" class="csl-entry">
Nobile, Agostino. 1998. <span>“A Hybrid Markov Chain for the Bayesian Analysis of the Multinomial Probit Model.”</span> <em>Statistics and Computing</em> 8.
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Per default, the first error-term variance is fixed to 1, i.e. <code>scale = list(&quot;parameter&quot; = &quot;s&quot;, &quot;index&quot; = 1, &quot;value&quot; = 1)</code>. Note that you can set <code>&quot;parameter&quot; = &quot;a&quot;</code> only if the model has parameters with a fixed coefficient (i.e. <code>P_f&gt;0</code>).<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>It is reasonable to wait a certain number of iterations before the next update to allow for readjustments, which is implemented via the <code>latent_classes$buffer</code> argument.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
</ol>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
